{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00a8b05",
   "metadata": {},
   "source": [
    "# **Governance Database Extraction and Preprocessing**\n",
    "\n",
    "Governance Database Proj retrieves data directly from OPENDART to build a database of KOSPI-listed corporations and executive status available through posted disclosures. \n",
    "\n",
    "For stability, information is pulled directly from OPENDART API where possible. Almost all OPENDART calls require a single API call per corporation or report request, which is reflected in the total execution time. Only one function relies on OpenDartReader (to search for direct links to audit committee information used for governance check). [OPENDART API limits](https://engopendart.fss.or.kr/cop/bbs/selectArticleDetail.do) are as follows: \n",
    "- Individual: 20,000 calls a day (the limit is for all 83 API services and not by service)\n",
    "- Corporation (business registration and registered IP)\n",
    "    - 2 services (\"Search disclosures\" and \"Overview of corporate status\"): Unlimited\n",
    "    - 81 services (excluding \"Search disclosures\" and \"Overview of corporate status\"): 20,000 calls a day (the limit is for all 81 API services and not by service)\n",
    "- 1,000 calls per minute\n",
    "\n",
    "### Environment Set Up \n",
    "Runs on Python 3.13.5. The cell below will check that the current kernel is using the correct Python version and raise an error otherwise. To set a virtual environment, execute the following lines in the terminal: \n",
    "\n",
    "    python3.13.5 -m venv virtualenv\n",
    "\n",
    "    virtualenv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23a744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 13, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca9e5e",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "\n",
    "By the end of the project, the following two databases will be produced: \n",
    "1. **executive_df**, providing details on the 15k+ listed executives, including information such as registered officer status, shareholder relations, salary, and professional experience.\n",
    "\n",
    "2. **summary_df**,  a grouped dataset across corp-level information, including number of directory types, audit committee size, and total assets from the past three years (used to determine audit committee mandate).\n",
    "\n",
    "*navigate to README.md file for reference*\n",
    "\n",
    "<br> \n",
    "The cell below checks for the necessary folders. If it returns False, create a folder (at the same directory level as notebooks, not within) labeled 'data' and within it, two subfolders: 'raw' and 'processed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f04f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(r'..\\..', 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "\n",
    "print(os.path.isdir(raw_dir) and os.path.isdir(processed_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701c639",
   "metadata": {},
   "source": [
    "### Packages \n",
    "\n",
    "Update **API_key**, **bsns_year**, and **reprt_code** as needed.\n",
    "\n",
    "OPENDART reprt_code: \n",
    "- First Quarterly Report : 11013\n",
    "- Semi-annual Report : 11012\n",
    "- Third Quarterly Report : 11014\n",
    "- Annual Report : 11011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b620bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import zipfile \n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "\n",
    "import dart_fss\n",
    "import OpenDartReader\n",
    "\n",
    "API_key = '0d67945133e224c451452e071e0d8349969353e1' \n",
    "\n",
    "dart = OpenDartReader(API_key)\n",
    "dart_fss.set_api_key(API_key)\n",
    "\n",
    "bsns_year = '2024'\n",
    "reprt_code = '11011'\n",
    "reference_date = datetime(2025, 8, 12) # used for tenure calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f819811",
   "metadata": {},
   "source": [
    "### **Data Extraction (data_extraction_ipynb)**\n",
    "\n",
    "Produces all the raw data files necessary for preprocessing. All 7 csv files are saved in the raw data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cc3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory exists at: ..\\..\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_DIR = os.path.join(r'..\\..', 'data', 'raw')\n",
    "os.makedirs(BASE_DATA_DIR, exist_ok=True)\n",
    "print(f\"Raw data directory exists at: {BASE_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5cbc9",
   "metadata": {},
   "source": [
    "#### 0. save_df_to_csv\n",
    "</b>\n",
    "\n",
    "a helper function, called at the end of each function to save outputs as csv files within raw data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4945b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_csv(df: pd.DataFrame, file_path: str, index: bool = False):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    try:\n",
    "        df.to_csv(file_path, index=index)\n",
    "        print(f\"DataFrame saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to CSV {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee79881",
   "metadata": {},
   "source": [
    "#### 1. get_corp_code\n",
    "</b>\n",
    "\n",
    "\n",
    "pulls the most up to date list of corp codes with a single API call to the OPENDART zip file. Corp codes are unique reference codes assigned by OPENDART, distinct from stock number and used as required keys to access and pull full company (2. get_kospi_company_info) and executive (3. get_executive_status_data) info. \n",
    "</b>\n",
    "\n",
    "[OPENDART | Guide for Developers to Corporation code](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE001&apiId=AE00004)\n",
    "</b>\n",
    "\n",
    "Required Key: \n",
    "</b>\n",
    "\n",
    "- crtfc_key (API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f963c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corp_code(api_key: str, output_dir: str = BASE_DATA_DIR) -> pd.DataFrame:\n",
    "    url_code = f'https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}'\n",
    "    response = requests.get(url_code) \n",
    "\n",
    "    # check that the target directory exists \n",
    "    os.makedirs('dart_data', exist_ok=True)\n",
    "\n",
    "    # unzip and extract CORPCODE.xml\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall('dart_data')\n",
    "        xml_path = os.path.join('dart_data', 'CORPCODE.xml')\n",
    "\n",
    "    # parse XML\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # filter for listed companies (6-digit stock code only) and append to df\n",
    "    corp_list = []\n",
    "    for corp in root.findall('list'):\n",
    "        stock_code = corp.findtext('stock_code')\n",
    "        if stock_code and len(stock_code) == 6:\n",
    "            corp_list.append({\n",
    "                'corp_code': corp.findtext('corp_code'),\n",
    "                'corp_name': corp.findtext('corp_name'),\n",
    "                'corp_eng_name': corp.findtext('corp_eng_name'),\n",
    "                'stock_code': stock_code\n",
    "            })\n",
    "\n",
    "    #save in raw data folder \n",
    "    corp_codes_df = pd.DataFrame(corp_list)\n",
    "    output_filepath = os.path.join(output_dir, 'listed_corp_codes.csv')\n",
    "    save_df_to_csv(corp_codes_df, output_filepath)\n",
    "    return corp_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8057e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ..\\..\\data\\raw\\listed_corp_codes.csv\n"
     ]
    }
   ],
   "source": [
    "all_corp_codes_df = get_corp_code(API_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fde21",
   "metadata": {},
   "source": [
    "#### 2. get_kospi_company_info\n",
    "</b>\n",
    "\n",
    "passes in the list of corp codes from get_corp_code, filters for kospi codes, and fetches all detailed company info. OPENDART requires an individual call for each *corp_code*, resulting in a total execution time of: ~ 9 minutes for 3,000+ calls.\n",
    "\n",
    "[OPENDART | Guide for Developers to Overview of corporate status](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE001&apiId=AE00002)\n",
    "\n",
    "Required Keys: \n",
    "- crtfc_key (API key) \n",
    "- corp_code \n",
    "\n",
    "Kept Data: \n",
    "</b>\n",
    "\n",
    "\n",
    "| Key  | Name | \n",
    "| -------|-----|\n",
    "| corp_name | Formal name\t  | \n",
    "| stock_code  | Stock item code\t  | \n",
    "| ceo_nm  | Representative name  |\n",
    "| induty_code*  |  Industry code   | \n",
    "\n",
    "*induty_code: not relevant now, could be used later to compare industry norms\n",
    "\n",
    "Dropped Data (corp code and name are sufficient for identification):\n",
    "| Key  | Name |\n",
    "| -------|-----|\n",
    "| corp_name_eng  | English name\t  | \n",
    "| stock_name | Item name \t  | \n",
    "| corp_cls  |  Corporation type   | \n",
    "| jurir_no  |  Corporate registration No.   |\n",
    "| bizr_no | Business registration No.  | \n",
    "| adres | Address  | \n",
    "| hm_url  | Website URL  | \n",
    "| ir_url  | IR website  | \n",
    "| phn_no |  Telephone No.   | \n",
    "| fax_no  | Fax No.  | \n",
    "| est_dt  | Establishment date (YYYYMMDD)  | \n",
    "| acc_mt  | Month of settlement (MM)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbf895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kospi_company_info(api_key: str, corp_codes_df: pd.DataFrame, output_dir: str = BASE_DATA_DIR) -> pd.DataFrame:\n",
    "    data = []\n",
    "    api_endpoint = \"https://engopendart.fss.or.kr/engapi/company.json\"\n",
    "\n",
    "    for i, row in corp_codes_df.iterrows(): # iterate over list of corp_codes\n",
    "        corp_code = row['corp_code']\n",
    "        corp_name = row['corp_name']\n",
    "\n",
    "        params = { # OPENDART required keys\n",
    "            'crtfc_key': API_key,\n",
    "            'corp_code': corp_code\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(api_endpoint, params=params)\n",
    "            response.raise_for_status() # raise HTTPError for bad responses \n",
    "            info = response.json()\n",
    "\n",
    "            # filter for KOSPI companies \n",
    "            if info and info.get('corp_cls') == 'Y': # all types: Y (KOSPI), K (KOSDAQ), N (KONEX), E (Other)\n",
    "                data.append({\n",
    "                    'corp_name': info.get('corp_name'),\n",
    "                    'corp_code': info.get('corp_code'),\n",
    "                    'stock_code': info.get('stock_code'),\n",
    "                    'ceo_name': info.get('ceo_nm'),\n",
    "                    'industry_code': info.get('induty_code'),\n",
    "                })\n",
    "            time.sleep(0.07) # respect API limit\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch company info for {corp_name} ({corp_code}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    # save in raw data folder\n",
    "    kospi_codes_df = pd.DataFrame(data)\n",
    "    output_filepath = os.path.join(output_dir, 'kospi_company_info.csv')\n",
    "    save_df_to_csv(kospi_codes_df, output_filepath)\n",
    "    return kospi_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67499e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ..\\..\\data\\raw\\kospi_company_info.csv\n"
     ]
    }
   ],
   "source": [
    "kospi_company_info_df = get_kospi_company_info(api_key=API_key, corp_codes_df=all_corp_codes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44028172",
   "metadata": {},
   "source": [
    "#### 3. get_executive_status_data\n",
    "\n",
    "retrieves executive-level data by passing in the list of KOSPI listed corps. OPENDART requires an individual API call per corporation, with 850 calls ~ 2 minutes.\n",
    "As the function iterates over the KOSPI corps, it flags any that have no executive data available.\n",
    "\n",
    "[OPENDART | Guide for Developers to Status of executives](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00011)\n",
    "\n",
    "Required Keys:\n",
    "- crtfc_key (API key)\n",
    "- corp_code \n",
    "- bsns_year (fiscal year)\n",
    "- reprt_code\n",
    "    - First Quarterly Report : 11013\n",
    "    - Semi-annual Report : 11012\n",
    "    - Third Quarterly Report : 11014\n",
    "    - Annual Report : 11011\n",
    "\n",
    "The resulting **executive_status_data_df** saves all the information available. In preprocessing, **exec_df** filters down based on the following: \n",
    "\n",
    "Kept Data: \n",
    "| Key  | Name | \n",
    "| -------|-----|\n",
    "| rcept_no | Filing No.  | \n",
    "| corp_cls | Corporation type\t  |\n",
    "| corp_code | Corporation code\t  | \n",
    "| corp_name | Corporation name\t  | \n",
    "| nm | Name  |\n",
    "| sexdstn | Gender  | \n",
    "| ofcps | Position  | \n",
    "| rgist_exctv_at | Registered officer status  | \n",
    "| fte_at | Full-time  | \n",
    "| chrg_job | Responsibilites  | \n",
    "| main_career | Professional Background  |\n",
    "| mxmm_shrholdr_relate | Relationship to Largest Shareholder  | \n",
    "| hffc_pd | Period of employment  | \n",
    "\n",
    "Dropped Data: \n",
    "| Key  | Name | \n",
    "| -------|-----|\n",
    "| birth_ym | Date of birth  | \n",
    "| tenure_end_on | Term expiration date  | \n",
    "| stlm_dt | Settlement date  | \n",
    "\n",
    "As tenure in company is sufficient for guaging expertise and *stlm_dt* is irrelevant given filter for year and report type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a31a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_executive_status_data(api_key: str, kospi_codes_df: pd.DataFrame, bsns_year: int, reprt_code: str, output_dir: str = os.path.join('data', 'raw')) -> pd.DataFrame:\n",
    "    results = []\n",
    "    api_endpoint = \"https://opendart.fss.or.kr/api/exctvSttus.json\"\n",
    "\n",
    "    for idx, row in kospi_codes_df.iterrows():\n",
    "        corp_code = row['corp_code']\n",
    "        corp_name = row['corp_name']\n",
    "        stock_code = row['stock_code']\n",
    "\n",
    "        params = { #OPENDART required keys\n",
    "            'crtfc_key': api_key,\n",
    "            'corp_code': corp_code,\n",
    "            'bsns_year': bsns_year,\n",
    "            'reprt_code': reprt_code\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_endpoint, params=params)\n",
    "            response.raise_for_status() # raise error for bad responses \n",
    "            data = response.json()\n",
    "\n",
    "            if data['status'] == '000': # success - data found\n",
    "                if 'list' in data and data['list']:\n",
    "                    df = pd.DataFrame(data['list']) # appends all info, later filtered down in preprocessing\n",
    "                    df['stock_code'] = stock_code\n",
    "                    \n",
    "                    results.append(df)\n",
    "            else:\n",
    "                print(f\"No executive data available for {corp_name} ({corp_code}) for {bsns_year}/{reprt_code}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for {corp_name} ({corp_code}): {e}\")\n",
    "\n",
    "        time.sleep(0.07) \n",
    "\n",
    "    if results:\n",
    "        executive_status_df = pd.concat(results, ignore_index=True)\n",
    "        output_filepath = os.path.join(output_dir, f'executive_status_{bsns_year}_{reprt_code}.csv')\n",
    "        save_df_to_csv(executive_status_df, output_filepath)\n",
    "        return executive_status_df\n",
    "    else:\n",
    "        print(\"\\nNo executive status data was retrieved.\")\n",
    "        return pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646dd740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executive data available for 미래에셋맵스 아시아퍼시픽 부동산공모 1호 투자회사 (00600013) for 2024/11011.\n",
      "No executive data available for 맥쿼리한국인프라투융자회사 (00435297) for 2024/11011.\n",
      "No executive data available for 한국투자ANKOR유전해외자원개발특별자산투자회사1호(지분증권) (00907013) for 2024/11011.\n",
      "No executive data available for 케이비발해인프라투융자회사 (01880801) for 2024/11011.\n",
      "No executive data available for 주식회사 대신밸류리츠위탁관리부동산투자회사 (01885222) for 2024/11011.\n",
      "No executive data available for 대한조선 주식회사 (00182696) for 2024/11011.\n",
      "DataFrame saved to data\\raw\\executive_status_2024_11011.csv\n"
     ]
    }
   ],
   "source": [
    "executive_status_data_df = get_executive_status_data(API_key, kospi_company_info_df, bsns_year, reprt_code) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbaf87",
   "metadata": {},
   "source": [
    "#### 4. get_total_assets  \n",
    "\n",
    "makes calls to OPENDART's financial statements API to pull total assets from each corp. For each corp, if a consolidated report ('CFS') exists, it pulls information from that statement. Otherwise, it falls back on the seperate report ('OFS'). The function supports pulling other FS data, so long as sj_div and sj_nm are located correctly, and *target_account_names* is updated to reflect the target key words. The full function makes 850 (length of KOSPI codes) calls, with execution time ~ 3 minutes.\n",
    "\n",
    "\n",
    "[OPENDART | Single company’s full financial statements 개발가이드](https://opendart.fss.or.kr/guide/detail.do?apiGrpCd=DE003&apiId=AE00036)\n",
    "\n",
    "Required Keys:\n",
    "- crtfc_key (API key)\n",
    "- corp_code \n",
    "- bsns_year (fiscal year)\n",
    "- reprt_code \n",
    "- fs_div (seperate/consolidated report)\n",
    "\n",
    "The resulting **assets_YYYY_REPORT**  will be used to check requirements for mandated audit committees (corporations with total assets > $2T KRW). Because corporations have a two year grace period for forming a mandated audit committee, the function pulls total assets from the past three years. \n",
    "\n",
    "Kept Data: \n",
    "| Key  | Name | \n",
    "| -------|-----|\n",
    "| rcept_no* | Filing No.  | \n",
    "| thstrm_amount\t| Term amount |\n",
    "| frmtrm_amount\t| Previous term amount | \n",
    "| bfefrmtrm_amount\t| Amount of term before previous | \n",
    "\n",
    "Dropped Data: \n",
    "| Key  | Name | \n",
    "| -------|-----|\n",
    "| reprt_code | Report code\t  | \n",
    "| bsns_year | Fiscal year\t  | \n",
    "| corp_code | Corporation code\t  | \n",
    "| sj_div** | Type of financial statement\t  |\n",
    "| sj_nm | Financial statement title\t  |\n",
    "| account_id | Account ID  |\n",
    "| account_nm | Account name  | \n",
    "| account_detail | Detail account  |\n",
    "| thstrm_nm\t| Term name  | \n",
    "| thstrm_add_amount\t | Accumulated term amount\t  | \n",
    "| frmtrm_nm\t| Previous term name | \n",
    "| frmtrm_q_nm | Previous term name(Quarterly/Semiannual) | \n",
    "| frmtrm_q_amount | Previous term amount(Quarterly/Semiannual) | \n",
    "| frmtrm_add_amount\t| Accumulated previous term amount  | \n",
    "| bfefrmtrm_nm\t| Name of term before previous | \n",
    "| ord\t| Account code sort order | \n",
    "| currency\t| Currency unit |\n",
    "\n",
    "*required key for subdoc searches in preprocessing\n",
    "\n",
    "**function already filters for sj_div = BS, can change to retrieve data from other statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ca9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_assets(kospi_company_info_df: pd.DataFrame, bsns_year: str, reprt_code: str, API_key: str, output_dir: str = os.path.join('data', 'raw')) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches Total Assets for a list of companies from the DART API without helper functions.\n",
    "    \"\"\"\n",
    "    api_url = 'https://opendart.fss.or.kr/api/fnlttSinglAcntAll.json'\n",
    "    target_sj_div = \"BS\"\n",
    "    target_account_names = {\"자산총계\", \"총자산\", \"자산\"} # checks for possible categories covering total assets \n",
    "    year = int(bsns_year)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for corp_code in kospi_company_info_df['corp_code']:\n",
    "        rcept_no, assets, prior_assets, two_years_ago = None, None, None, None\n",
    "        \n",
    "        # try CFS first, fall back on OFS\n",
    "        for fs_div in ['CFS', 'OFS']:\n",
    "            params = {'crtfc_key': API_key, 'corp_code': corp_code, 'bsns_year': bsns_year, 'reprt_code': reprt_code, 'fs_div': fs_div}\n",
    "            try:\n",
    "                res = requests.get(api_url, params=params)\n",
    "                res.raise_for_status()\n",
    "                data = res.json()\n",
    "                \n",
    "                if data.get('status') == '000' and 'list' in data:\n",
    "                    # search for assets data directly from the JSON list\n",
    "                    for item in data['list']:\n",
    "                        if item['sj_div'] == target_sj_div and item['account_nm'].strip().replace(' ', '') in target_account_names:\n",
    "                            rcept_no = item.get('rcept_no')\n",
    "                            assets = pd.to_numeric(item.get('thstrm_amount', '').replace(',', ''), errors='coerce')\n",
    "                            prior_assets = pd.to_numeric(item.get('frmtrm_amount', '').replace(',', ''), errors='coerce')\n",
    "                            two_years_ago = pd.to_numeric(item.get('bfefrmtrm_amount', '').replace(',', ''), errors='coerce')\n",
    "                            break \n",
    "                    \n",
    "                    if assets is not None:\n",
    "                        break \n",
    "            \n",
    "            except (requests.exceptions.RequestException, ValueError):\n",
    "                continue\n",
    "        \n",
    "        if assets is None:\n",
    "            print(f\"Total Assets not found for {corp_code}.\")\n",
    "\n",
    "        all_results.append({\n",
    "            'corp_code': corp_code,\n",
    "            'rcept_no': rcept_no,\n",
    "            f'{year}_total_assets': assets,\n",
    "            f'{year - 1}_total_assets': prior_assets,\n",
    "            f'{year - 2}_total_assets': two_years_ago\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.07)\n",
    "\n",
    "    assets_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    for y_offset in [0, 1, 2]:\n",
    "        col_name = f'{year - y_offset}_total_assets'\n",
    "        if col_name in assets_df.columns:\n",
    "            assets_df[col_name] = pd.to_numeric(assets_df[col_name], errors='coerce').astype('Int64')\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"assets_{bsns_year}_{reprt_code}.csv\")\n",
    "    assets_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    return assets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2dca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Assets not found for 00600013.\n",
      "Total Assets not found for 00435297.\n",
      "Total Assets not found for 00907013.\n",
      "Total Assets not found for 01880801.\n",
      "Total Assets not found for 00112998.\n",
      "Total Assets not found for 01885222.\n",
      "Total Assets not found for 00182696.\n"
     ]
    }
   ],
   "source": [
    "assets_df = get_total_assets(kospi_company_info_df, bsns_year, reprt_code, API_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bd84d",
   "metadata": {},
   "source": [
    "#### 5. get_salary_type\n",
    "\n",
    "pulls salary data from three OPDENDART source types: \n",
    "- Individual, which discloses the exact amount for executives making more than 500M KRW. \n",
    "- Grouped, which provides total annual grouped salary and average salaries by status type. \n",
    "- Unregistered, which provides the total annual grouped salary and average per person.\n",
    "\n",
    "In the preprocessing notebook, salary is appended to each executive - exact where possible and average amounts otherwise. Because there are three separate API endpoints, **get_salary_type** contains two internal helped functions: **_get_json** and **_get_salary_data_for_corp**. When the main function is called, the list of KOSPI *corp_codes* is passed as keys to **_get_salary_data_for_corp**, which makes separate calls to each source type using **_get_json**. All the disclosed datapoints are then appended larger, consolidated dataframe saved as **salary_data_YYYY_REPORT** in the raw data folder, and refered to within this notebook as **salary_separate_df**. If any errors occur in retrieving a corp's data, the details will be flagged and printed. Each corp code makes 3 API requests, totaling 2550 executing in ~ 4 minutes.\n",
    "\n",
    "1. [OPENDART | Guide for Developers to Remuneration for individual directors and auditors](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00013) (lists all those > 500m KRW)\n",
    "\n",
    "2. [OPENDART | Guide for Developers to Remuneration for all directors and auditors (remuneration paid - by type)](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00030)\n",
    "\n",
    "3. [OPENDART | Guide for Developers to Remuneration for unregistered executives](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00028)\n",
    "\n",
    "Required Keys:\n",
    "- crtfc_key (API key)\n",
    "- corp_code \n",
    "- bsns_year (fiscal year)\n",
    "- reprt_code \n",
    "\n",
    "The resulting **salary_separate_df** displays the salary data pulled from the three datapoints, with standardized columns for the purposes of merging with **exec_df** in the preprocessing notebook. The following chart maps the resulting **salary_separate_df**'s column names to the corresponding OPENDART source.\n",
    "\n",
    "| salary_separate_df column  | (1.) Individual | (2.) All By Type | (3.) Unregistered | \n",
    "| -------|-----|-----|-----|\n",
    "| position | ofcps | se (category)* | se\t(unregistered) |\n",
    "| compensation | mendng_totamt | psn1_avrg_pymntamt | jan_salary_am |\n",
    "| salary_source | 개인별보수 | 임원전체보수유형 | 미등기임원 |\n",
    "| salary_type | exact | estimate | estimate | \n",
    "\n",
    "\n",
    "*Category covers:\n",
    "\n",
    "- Registered director (excluding outside directors and members of the audit committee)\n",
    "- Outside director (excluding members of the audit committee)\n",
    "- Member of the audit committee\n",
    "- Auditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e8c5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_type(kospi_company_info_df: pd.DataFrame, bsns_year: str, reprt_code: str, API_key: str, output_dir: str = os.path.join('data', 'raw')) -> pd.DataFrame:\n",
    "    # === Internal Helper Function: DART API JSON request ===\n",
    "    def _get_json(url, corp_code):\n",
    "        \"\"\"Helper to fetch JSON data from DART API and handle errors.\"\"\"\n",
    "        params = {\n",
    "            'crtfc_key': API_key,\n",
    "            'corp_code': corp_code,\n",
    "            'bsns_year': bsns_year,\n",
    "            'reprt_code': reprt_code\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') != '000' or 'list' not in data:\n",
    "                if data.get('status') != '000':\n",
    "                    print(f\"OPENDART Error for {corp_code}: {data.get('message')}\")\n",
    "                return []\n",
    "            return data['list']\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {url} with params {params}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # === Internal Helper Function: Fetches data for a single company ===\n",
    "    def _get_salary_data_for_corp(corp_code):\n",
    "        \"\"\"Fetches and consolidates salary data for a single company.\"\"\"\n",
    "        endpoints = {\n",
    "            'individual': 'https://opendart.fss.or.kr/api/hmvAuditIndvdlBySttus.json',\n",
    "            'unregistered': 'https://opendart.fss.or.kr/api/unrstExctvMendngSttus.json',\n",
    "            'grouped': 'https://opendart.fss.or.kr/api/drctrAdtAllMendngSttusMendngPymntamtTyCl.json' \n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        # 1. Individual executives (개인별 보수)\n",
    "        for row in _get_json(endpoints['individual'], corp_code):\n",
    "            results.append({\n",
    "                'corp_code': corp_code,\n",
    "                'name': row.get('nm'),\n",
    "                'position': row.get('ofcps'),\n",
    "                'compensation': row.get('mendng_totamt'), \n",
    "                'salary_source': '개인별보수',\n",
    "                'salary_type': 'exact'\n",
    "            })\n",
    "\n",
    "        # 2. Unregistered executives (미등기 임원)\n",
    "        for row in _get_json(endpoints['unregistered'], corp_code):\n",
    "            results.append({\n",
    "                'corp_code': corp_code,\n",
    "                'name': '',\n",
    "                'position': row.get('se'),\n",
    "                'compensation': row.get('jan_salary_am'), \n",
    "                'salary_source': '미등기임원',\n",
    "                'salary_type': 'estimate'\n",
    "            })\n",
    "\n",
    "        # 3. Grouped executives (임원 전체 보수 유형)\n",
    "        for row in _get_json(endpoints['grouped'], corp_code):\n",
    "            results.append({\n",
    "                'corp_code': corp_code,\n",
    "                'name': '',\n",
    "                'position': row.get('se'),\n",
    "                'compensation': row.get('psn1_avrg_pymntamt'),\n",
    "                'salary_source': '임원전체보수유형',\n",
    "                'salary_type': 'estimate'\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    # === Main loop to process all companies ===\n",
    "    all_salary_data = []\n",
    "\n",
    "    for corp_code in kospi_company_info_df['corp_code'].apply(lambda c: str(c).zfill(8)):\n",
    "        df = _get_salary_data_for_corp(corp_code)\n",
    "        \n",
    "        if not df.empty:\n",
    "            all_salary_data.append(df)\n",
    "        \n",
    "        # Respect DART API rate limits\n",
    "        time.sleep(0.07)\n",
    "\n",
    "    # Concatenate all individual DataFrames into one\n",
    "    final_df = pd.concat(all_salary_data, ignore_index=True)\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"salary_separate_{bsns_year}_{reprt_code}.csv\")\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Salary data for {len(all_salary_data)} companies saved to: {output_path}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd99ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENDART Error for 00600013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00600013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00600013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00435297: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00435297: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00435297: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00907013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00907013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00907013: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01880801: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01880801: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01880801: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01885222: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01885222: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 01885222: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00182696: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00182696: 조회된 데이타가 없습니다.\n",
      "OPENDART Error for 00182696: 조회된 데이타가 없습니다.\n",
      "Salary data for 844 companies saved to: data\\raw\\salary_separate_2024_11011.csv\n"
     ]
    }
   ],
   "source": [
    "salary_separate_df = get_salary_type(kospi_company_info_df, bsns_year, reprt_code, API_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945f624",
   "metadata": {},
   "source": [
    "#### 6. get_salary_total  \n",
    "\n",
    "pulls the total salary for each corp. \n",
    "\n",
    "[OPENDART | Guide for Developers to Remuneration for all directors and auditors](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00014)\n",
    "\n",
    "Required Keys:\n",
    "- crtfc_key (API key)\n",
    "- corp_code \n",
    "- bsns_year (fiscal year)\n",
    "- reprt_code \n",
    "\n",
    "The resulting **salary_total_df** keeps all response variables. This includes: \n",
    "- nmpr (total headcount of all directors and auditors)\n",
    "- mendng_totamt (total remuneration amount for all directors and auditors)\n",
    "- jan_avrg_mendng_am (the average remuneration per person)\n",
    "\n",
    "These points are used to check **summary_df** values, to ensure that total headcount and remuneration totals align. The total remuneration amount is then merged with **summary_df** as a *Total Compensation* column. The functions makes ~850 calls and executes in ~ 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4020c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_total(kospi_company_info_df: pd.DataFrame, bsns_year: str, reprt_code: str, API_key: str, output_dir: str = os.path.join('data', 'raw')) -> pd.DataFrame:\n",
    "    url = \"https://opendart.fss.or.kr/api/hmvAuditAllSttus.json\"\n",
    "\n",
    "    # === Internal Helper Function: DART API JSON request ===\n",
    "    def _get_json(url, corp_code):\n",
    "        params = {\n",
    "            'crtfc_key': API_key,\n",
    "            'corp_code': corp_code,\n",
    "            'bsns_year': bsns_year,\n",
    "            'reprt_code': reprt_code\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('status') != '000' or 'list' not in data:\n",
    "                if data.get('status') != '000':\n",
    "                    print(f\"DART API Error for {corp_code}: {data.get('message')}\")\n",
    "                return []\n",
    "            return data['list']\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {url} with params {params}: {e}\")\n",
    "            return []\n",
    "        \n",
    "    salary_total = []\n",
    "    \n",
    "    for corp_code in kospi_company_info_df['corp_code'].apply(lambda c: str(c).zfill(8)):\n",
    "        data_list = _get_json(url, corp_code)\n",
    "\n",
    "        if data_list:\n",
    "            df = pd.DataFrame(data_list)\n",
    "            salary_total.append(df)\n",
    "        \n",
    "        time.sleep(0.07)\n",
    "        \n",
    "    # concatenate all individual DataFrames into one\n",
    "    final_df = pd.concat(salary_total, ignore_index=True)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"salary_total_data_{bsns_year}_{reprt_code}.csv\")\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Salary data for {len(salary_total)} companies saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f76336d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DART API Error for 00600013: 조회된 데이타가 없습니다.\n",
      "DART API Error for 00435297: 조회된 데이타가 없습니다.\n",
      "DART API Error for 00907013: 조회된 데이타가 없습니다.\n",
      "DART API Error for 01880801: 조회된 데이타가 없습니다.\n",
      "DART API Error for 01885222: 조회된 데이타가 없습니다.\n",
      "DART API Error for 00182696: 조회된 데이타가 없습니다.\n",
      "Salary data for 844 companies saved to: data\\raw\\salary_total_data_2024_11011.csv\n"
     ]
    }
   ],
   "source": [
    "salary_total_df = get_salary_total(kospi_company_info_df, bsns_year, reprt_code, API_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f09ca0",
   "metadata": {},
   "source": [
    "#### 7. get_major_shareholder_data\n",
    "\n",
    "pulls holding status of major shareholders. \n",
    "\n",
    "In the preprocessing notebook, shareholder status is merged with the exec data, such that if a registered or unregistered executive is listed as a major shareholder, their shares are appended to exec_df.\n",
    "\n",
    "[OPENDART | Guide for Developers to Information on largest shareholder](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE002&apiId=AE00008)\n",
    "\n",
    "Required Keys:\n",
    "- crtfc_key (API key)\n",
    "- corp_code \n",
    "- bsns_year (fiscal year)\n",
    "- reprt_code \n",
    "\n",
    "The resulting **major_shareholder_df** contains all response keys for potential further evaluation. When merged to **exec_df**, only *trmend_posesn_stock_qota_rt* (shareholding ratio at the end of the reporting period) is added as a *Shareholding Ratio* column. The following are not carried over: \n",
    "- bsis_posesn_stock_co\t(number of stocks at the beginning of the reporting period)\n",
    "- bsis_posesn_stock_qota_rt (shareholding ratio at the beginning of the reporting period)\n",
    "- trmend_posesn_stock_co (number of stocks at the end of the reporting period)\n",
    "\n",
    "Alternative Source: [OPENDART | Guide for Developers to Report of executives and major shareholders' ownership](https://engopendart.fss.or.kr/guide/detail.do?apiGrpCd=DE004&apiId=AE00041) which pulls stock transaction updates by executives and major shareholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "271242e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_major_shareholder_data(api_key: str, kospi_codes_df: pd.DataFrame, bsns_year: int, reprt_code: str, output_dir: str = os.path.join('data', 'raw')) -> pd.DataFrame:\n",
    "    results = []\n",
    "    api_endpoint = \"https://opendart.fss.or.kr/api/hyslrSttus.json\"\n",
    "    total_corps = len(kospi_codes_df)\n",
    "\n",
    "    for idx, row in kospi_codes_df.iterrows():\n",
    "        corp_code = row['corp_code']\n",
    "        corp = str(corp_code).zfill(8)\n",
    "        corp_name = row['corp_name']\n",
    "\n",
    "        params = {\n",
    "            'crtfc_key': api_key,\n",
    "            'corp_code': corp,\n",
    "            'bsns_year': bsns_year,\n",
    "            'reprt_code': reprt_code\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data['status'] == '000':\n",
    "                if 'list' in data and data['list']:\n",
    "                    df = pd.DataFrame(data['list'])\n",
    "                    results.append(df)\n",
    "            elif data['status'] == '013':\n",
    "                print(f\"No shareholder data available for {corp_name} ({corp_code}) for {bsns_year}/{reprt_code}.\")\n",
    "            else:\n",
    "                print(f\"API Error for {corp_name} ({corp_code}): Status {data.get('status')}, Message: {data.get('message')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for {corp_name} ({corp_code}): {e}\")\n",
    "\n",
    "        time.sleep(0.07)\n",
    "\n",
    "    if results:\n",
    "        shareholder_df = pd.concat(results, ignore_index=True)\n",
    "        output_filepath = os.path.join(output_dir, f'major_shareholders_{bsns_year}_{reprt_code}.csv')\n",
    "        save_df_to_csv(shareholder_df, output_filepath)\n",
    "        print(f\"\\nSuccessfully fetched and saved major shareholder data for {len(shareholder_df)} records.\")\n",
    "        return shareholder_df\n",
    "    else:\n",
    "        print(\"\\nNo major shareholder data was retrieved.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ce067eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shareholder data available for 미래에셋맵스 아시아퍼시픽 부동산공모 1호 투자회사 (00600013) for 2024/11011.\n",
      "No shareholder data available for 맥쿼리한국인프라투융자회사 (00435297) for 2024/11011.\n",
      "No shareholder data available for 한국투자ANKOR유전해외자원개발특별자산투자회사1호(지분증권) (00907013) for 2024/11011.\n",
      "No shareholder data available for 케이비발해인프라투융자회사 (01880801) for 2024/11011.\n",
      "No shareholder data available for 주식회사 대신밸류리츠위탁관리부동산투자회사 (01885222) for 2024/11011.\n",
      "No shareholder data available for 대한조선 주식회사 (00182696) for 2024/11011.\n",
      "DataFrame saved to data\\raw\\major_shareholders_2024_11011.csv\n",
      "\n",
      "Successfully fetched and saved major shareholder data for 9102 records.\n"
     ]
    }
   ],
   "source": [
    "major_shareholder_df = get_major_shareholder_data(API_key, kospi_company_info_df, bsns_year, reprt_code) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f16c5",
   "metadata": {},
   "source": [
    "### **Data Extraction (data_extraction_ipynb)**\n",
    "\n",
    "Groups and concatonates data into two data dataframes: **exec_df** and **summary _df**. Unlike *data_extraction*, only the final cleaned and completed dfs will be saved to the processed data folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0fea6",
   "metadata": {},
   "source": [
    "#### 0. Build initial exec_df structure\n",
    "\n",
    "From the raw data file **executive_status_data_df**, the following cell drops the columns identified in *data extraction* (3.) get_executive_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f674c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_df = executive_status_data_df.drop(\n",
    "    columns=['corp_cls', 'birth_ym', 'fte_at', 'tenure_end_on', 'stlm_dt'], \n",
    "    errors='ignore'\n",
    ").rename(\n",
    "    columns={\n",
    "        'rcept_no': 'disclosure',\n",
    "        'nm': 'name',\n",
    "        'sexdstn': 'gender',\n",
    "        'ofcps': 'position',\n",
    "        'rgist_exctv_at': 'exec_status',\n",
    "        'chrg_job': 'responsibilities',\n",
    "        'mxmm_shrholdr_relate': 'largest_shareholder_relate',\n",
    "        'hffc_pd': 'employment_period',\n",
    "        'trmend_posesn_stock_qota_rt': 'shareholding_ratio'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd801cc",
   "metadata": {},
   "source": [
    "#### **exec_df**\n",
    "#### 1. Parse Experience and Build Initial Structure\n",
    "\n",
    "separate_career passes in the prior_work column in **exec_df** to parse and categorize into education and work experience. The function prioritizes sorting work related roles first, to avoid education related positions. Job keywords only contains these related terms as the fallback defaults to work experience if the specific education keywords don't exist within each parsed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32165a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_career(career_string):\n",
    "    if pd.isna(career_string):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    education = []\n",
    "    work_experience = []\n",
    "    \n",
    "    # prioritized keywords\n",
    "    job_keywords = ['교수', '총장', '강사', '연구원', '학장', '팀장', '실장', '감사', '대표', '회장', '이사']\n",
    "    edu_keywords = ['학사', '석사', '박사', '대학교', '법학', '대학원', '졸업', '수료', 'Univ.', 'School', 'College', 'MBA', 'U.', 'Institute', 'University']\n",
    "\n",
    "    career_items = career_string.split('\\n')\n",
    "    \n",
    "    for item in career_items:\n",
    "        # check for job keywords for education-related backgrounds to avoid sorting as education \n",
    "        if any(keyword in item for keyword in job_keywords):\n",
    "            work_experience.append(item.strip())\n",
    "        # if no job keywords, check for educational keywords\n",
    "        elif any(keyword in item for keyword in edu_keywords):\n",
    "            education.append(item.strip())\n",
    "        # default to work experience for other entries\n",
    "        else:\n",
    "            work_experience.append(item.strip())\n",
    "            \n",
    "    return (\n",
    "        education if education else np.nan,\n",
    "        work_experience if work_experience else np.nan\n",
    "    )\n",
    "exec_df[['education', 'work_exp']] = exec_df['main_career'].apply(\n",
    "    lambda x: pd.Series(separate_career(x))\n",
    ")\n",
    "\n",
    "# drop the old career column and reorder the columns \n",
    "exec_df = exec_df.drop(columns=['main_career']).pipe(\n",
    "    lambda df: df[['stock_code'] + [col for col in df.columns if col != 'stock_code']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc21f1",
   "metadata": {},
   "source": [
    "#### 2. Individual Audit Committee Membership\n",
    "\n",
    "The following code block extracts audit committee membership and auditor status from individual **exec_df** rows, in order for proper counting in **summary_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71337c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_and_split(responsibility_string):\n",
    "    if not isinstance(responsibility_string, str):\n",
    "        return []\n",
    "    \n",
    "    # split the string by newlines, strip whitespace, and filter out empty strings\n",
    "    return [item.strip() for item in responsibility_string.split('\\n') if item.strip()]\n",
    "\n",
    "exec_df['responsibilities'] = exec_df['responsibilities'].apply(_clean_and_split)\n",
    "\n",
    "def is_audit_committee_member(responsibilities):\n",
    "    if not responsibilities:\n",
    "        return False\n",
    "    # check if any item in the list matches the audit committee pattern\n",
    "    for responsibility in responsibilities:\n",
    "        responsibility_cleaned = re.sub(r'\\s', '', responsibility)\n",
    "        if re.search(r'감사위원회위원|감사위원|감사위원장', responsibility_cleaned):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_auditor_exclusive(responsibilities):\n",
    "    if not responsibilities:\n",
    "        return False\n",
    "    # first, check if the person is an audit committee member and return False if they are\n",
    "    if is_audit_committee_member(responsibilities):\n",
    "        return False\n",
    "    \n",
    "    # otherwise, check for the isolated auditor pattern\n",
    "    for responsibility in responsibilities:\n",
    "        responsibility_cleaned = re.sub(r'\\s', '', responsibility)\n",
    "        if '감사' in responsibility_cleaned and not re.search(r'감사위원회위원|감사위원', responsibility_cleaned):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# apply the functions directly, performing the cleaning inside the lambda\n",
    "exec_df['is_audit_committee_member'] = exec_df['responsibilities'].apply(\n",
    "    lambda x: is_audit_committee_member(_clean_and_split(x))\n",
    ")\n",
    "\n",
    "exec_df['is_auditor'] = exec_df['responsibilities'].apply(\n",
    "    lambda x: is_auditor_exclusive(_clean_and_split(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a58a6",
   "metadata": {},
   "source": [
    "#### 3. Assign Compensation\n",
    "\n",
    "assign_compensation appends the estimated/exact reported salary based on the executive's registered status and audit committee membership. \n",
    "\n",
    "It prioritizes the exact labels and filters the categorization of the executive based on the status listed in the estimated groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c819288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_compensation(exec_df: pd.DataFrame, salary_type_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    exec_df['salary'] = None\n",
    "    exec_df['salary_source'] = None\n",
    "    exec_df['salary_type'] = None\n",
    "\n",
    "    for idx, row in exec_df.iterrows():\n",
    "        corp_code = row['corp_code']\n",
    "        name = row['name']\n",
    "        status = row.get('exec_status')\n",
    "        is_auditor = row.get('is_auditor', False)\n",
    "        is_committee = row.get('is_audit_committee_member', False)\n",
    "\n",
    "        # 1. Try to match by name\n",
    "        match = salary_separate_df[\n",
    "            (salary_separate_df['corp_code'] == corp_code) & \n",
    "            (salary_separate_df['name'] == name)\n",
    "        ]\n",
    "\n",
    "        if not match.empty:\n",
    "            row_data = match.iloc[0]\n",
    "\n",
    "        else:\n",
    "            if status == '미등기':\n",
    "                label = '미등기임원' \n",
    "            # 2. Estimate fallback: build label\n",
    "            elif is_auditor:\n",
    "                label = '감사'\n",
    "            elif status == '사외이사':\n",
    "                label = '감사위원회 위원' if is_committee else '사외이사(감사위원회 위원 제외)'\n",
    "            else:\n",
    "                label = '등기이사(사외이사, 감사위원회 위원 제외)'\n",
    "\n",
    "            group_match = salary_separate_df[\n",
    "                (salary_separate_df['corp_code'] == corp_code) & \n",
    "                (salary_separate_df['name'].isna()) & \n",
    "                (salary_separate_df['position'] == label)\n",
    "            ]\n",
    "\n",
    "            row_data = group_match.iloc[0] if not group_match.empty else pd.Series(dtype='object')\n",
    "\n",
    "        # 3. Assign if valid\n",
    "        if not row_data.empty:\n",
    "            exec_df.at[idx, 'salary'] = row_data.get('compensation')\n",
    "            exec_df.at[idx, 'salary_source'] = row_data.get('salary_source')\n",
    "            exec_df.at[idx, 'salary_type'] = row_data.get('salary_type')\n",
    "\n",
    "    return exec_df\n",
    "\n",
    "exec_df = assign_compensation(exec_df, salary_separate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c76624",
   "metadata": {},
   "source": [
    "#### 4. Merge Shareholder Data\n",
    "\n",
    "merges **exec_df** with **major_shareholder_df** to shareholding ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8b60910",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_df = pd.merge(exec_df, major_shareholder_df[['corp_code', 'nm', 'trmend_posesn_stock_qota_rt']], \n",
    "                   left_on=['corp_code', 'name'], right_on=['corp_code', 'nm'], how='left')\n",
    "exec_df = exec_df.drop(columns=['nm']).rename(\n",
    "    columns={'trmend_posesn_stock_qota_rt': 'shareholding_ratio'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c2645",
   "metadata": {},
   "source": [
    "#### 5. Standardize Tenure\n",
    "\n",
    "merges **exec_df** with **major_shareholder_df** to shareholding ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2657458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tenure_to_months(tenure_str, current_date=None):\n",
    "    if pd.isna(tenure_str) or not isinstance(tenure_str, str) or not tenure_str.strip():\n",
    "        return pd.NA\n",
    "        \n",
    "    tenure_str = tenure_str.strip()\n",
    "    \n",
    "    # 1. all date formats, including those with extra text\n",
    "    date_match = re.search(r'(\\d{2,4}[년\\.\\s]\\d{1,2}[월]?(?:[년\\.\\s]\\d{1,2}[일])?|\\d{1,2}\\.\\d{1,2}\\.\\d{1,2})', tenure_str)\n",
    "    \n",
    "    if date_match:\n",
    "        date_str = date_match.group(1).replace(' ', '').replace('년', '.').replace('월', '').replace('일', '')\n",
    "        date_obj = pd.NaT\n",
    "        \n",
    "        # parse the cleaned date string with multiple formats\n",
    "        date_formats = ['%Y.%m.%d', '%Y.%m', '%d.%m.%y', '%y.%m.%d', '%y.%m']\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                date_obj = pd.to_datetime(date_str, format=fmt, errors='raise')\n",
    "                break # Exit the loop if parsing is successful\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "    \n",
    "        if pd.notna(date_obj):\n",
    "            if current_date is None:\n",
    "                current_date = datetime.now()\n",
    "            \n",
    "            total_months = (current_date.year - date_obj.year) * 12 + (current_date.month - date_obj.month)\n",
    "            return float(max(0, total_months))\n",
    "    \n",
    "    # 2. decimal years (e.g., '11.3년', '22.5')\n",
    "    match_deci = re.search(r'^(\\d+(?:\\.\\d+)?)(?:년)?$', tenure_str)\n",
    "    if match_deci:\n",
    "        decimal_years = float(match_deci.group(1))\n",
    "        return decimal_years * 12\n",
    "    \n",
    "    # 3. years and months (e.g., \"3년 6개월\", \"4년4개월\")\n",
    "    match_ym = re.search(r'(\\d+)\\s*년(?:[^\\d]+)?\\s*(\\d+)\\s*개월', tenure_str)\n",
    "    if match_ym:\n",
    "        years = int(match_ym.group(1))\n",
    "        months = int(match_ym.group(2))\n",
    "        return float(years * 12 + months)\n",
    "        \n",
    "    # 4. years only (e.g., \"3년\")\n",
    "    match_y = re.search(r'^(\\d+)\\s*년$', tenure_str)\n",
    "    if match_y:\n",
    "        years = int(match_y.group(1))\n",
    "        return float(years * 12)\n",
    "        \n",
    "    # 5. months only (e.g., \"18개월\")\n",
    "    match_m = re.search(r'^(\\d+)\\s*개월$', tenure_str)\n",
    "    if match_m:\n",
    "        months = int(match_m.group(1))\n",
    "        return float(months)\n",
    "    \n",
    "    return pd.NA\n",
    "\n",
    "exec_df['employment_period'] = exec_df['employment_period'].apply(\n",
    "    lambda x: convert_tenure_to_months(x, current_date=reference_date) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60faa94",
   "metadata": {},
   "source": [
    "#### **summary_df**\n",
    "\n",
    "#### 1. Build Initial summary_df\n",
    "\n",
    "groups and summarizes **exec_df** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_optimized(group):\n",
    "    voting_directors_group = group[~group['exec_status'].isin(['미등기', '감사'])]\n",
    "    female_voting = (voting_directors_group['gender'] == '여').sum()\n",
    "    male_voting = (voting_directors_group['gender'] == '남').sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'audit_committee': group['is_audit_committee_member'].sum(),\n",
    "        'audit_committee_ods': ((group['is_audit_committee_member']) & (group['exec_status'] == '사외이사')).sum(),\n",
    "        'inside_directors': group['exec_status'].isin(['사내이사', '대표집행임원']).sum(),\n",
    "        'outside_directors': (group['exec_status'] == '사외이사').sum(),\n",
    "        'female_voting': female_voting,\n",
    "        'male_voting': male_voting,\n",
    "        'voting_directors': female_voting + male_voting,\n",
    "        'other_non_exec_directors': (group['exec_status'] == '기타비상무이사').sum(),\n",
    "        'auditors': group['is_auditor'].sum(),\n",
    "        'non_registered': (group['exec_status'] == '미등기').sum()\n",
    "    })\n",
    "\n",
    "summary_df = exec_df.groupby(['stock_code', 'corp_code', 'corp_name'], as_index=False).apply(\n",
    "    extract_summary_optimized, include_groups=False\n",
    ")\n",
    "\n",
    "summary_df = pd.merge(\n",
    "    summary_df,\n",
    "    assets_df[['corp_code', '2024_total_assets', '2023_total_assets', '2022_total_assets']],\n",
    "    on='corp_code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "disclosure = exec_df.groupby('corp_code')['disclosure'].max().reset_index()\n",
    "summary_df = pd.merge(\n",
    "    summary_df,\n",
    "    disclosure,\n",
    "    on='corp_code',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752861ef",
   "metadata": {},
   "source": [
    "#### 2. Audit Committee Checks\n",
    "\n",
    "For **summary_df**, each corp goes through two rounds of governance checks (*audit_committee_compliance*). After the first pass, to make sure that the flagged corporations are due to actual discrepencies and not incomplete data that was originally reported and pulled from OPENDART's exec_df (where for example, an outside committee member is not listed as an executive but does hold an active position) will parse the corp's financial statement, correct missing data, and run through the check again. To access the audit committee details directly, **missing_acm_urls(flagged_df)** will pull the relevant url from opendart reader's subdocs function. The direct link to the audit committee file will then be parsed through in **parse_and_update_audit_members(audit_targets_df, exec_df, summary_df)** function. If the **summary_df** data on audit committee size and membership doesn't match what's listed on the financial document directly, the function will update **summary_df**. On the second pass, corps that fail the governance checks will be flagged, alongside their failed condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd359c95",
   "metadata": {},
   "source": [
    "##### 2A. check_governance_compliance\n",
    "\n",
    "runs checks on **summary_df**. Checks for the following: \n",
    "\n",
    "1. Mandated Audit Committee: If a corporation's total assets > 2T KRW, an audit committee exists. As corporations have a 2 year grace period, the function passes in the total asset value from 2 years prior. If a corporation has no reported total assets for that year, it falls back back on the year after. \n",
    "2. Outside Majority: If a mandated audit committee exists, outside directors must make up a majority of the acting members. \n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9545e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_committee_compliance(df):\n",
    "    \n",
    "    # 1. prioritize total assets from 2 years ago, otherwise use the first available to identify large corps\n",
    "    df['total_assets'] = df['2022_total_assets'].fillna(df['2023_total_assets']).fillna(df['2024_total_assets'])\n",
    "    is_large_company = df['total_assets'] > 2_000_000_000_000\n",
    "    \n",
    "    large_corps = df.loc[is_large_company].copy()\n",
    "    \n",
    "    # 2. check compliance rules for large corps \n",
    "\n",
    "    # flag 1: if there is no audit committee, or if the audit committee has less than 3 members \n",
    "    large_corps['audit_committee_fail'] = (large_corps['audit_committee'] < 3)\n",
    "    \n",
    "    # flag 2: outside directors don't make up a 2/3 majority of the audit committee\n",
    "    large_corps['committee_majority_fail'] = (\n",
    "        large_corps['audit_committee_ods'] <= (2/3) * large_corps['audit_committee']\n",
    "    )\n",
    "    \n",
    "    # flag 3: there are less than 3 outside directors in total\n",
    "    large_corps['outside_minimum_fail'] = (\n",
    "        large_corps['outside_directors'] < 3\n",
    "    )\n",
    "    \n",
    "    def get_failure_messages(row):\n",
    "        messages = []\n",
    "        if pd.notna(row['audit_committee_fail']) and row['audit_committee_fail']:\n",
    "            messages.append(f\"Audit Committee has fewer than 3 members ({row['audit_committee']}) or none listed.\")\n",
    "        if pd.notna(row['committee_majority_fail']) and row['committee_majority_fail']:\n",
    "            messages.append(f\"Audit Committee Outside Directors ({row['audit_committee_ods']}) < 2/3 of Audit Committee ({row['audit_committee']}).\")\n",
    "        if pd.notna(row['outside_minimum_fail']) and row['outside_minimum_fail']:\n",
    "            messages.append(f\"Fewer than 3 Outside Directors ({row['outside_directors']}).\")\n",
    "        return \"; \".join(messages)\n",
    "\n",
    "    large_corps['flags'] = large_corps.apply(get_failure_messages, axis=1)\n",
    "    \n",
    "    return large_corps.loc[large_corps['flags'] != ''].drop(columns=[\n",
    "        'total_assets', 'audit_committee_fail', 'committee_majority_fail',\n",
    "        'outside_minimum_fail'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0beb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged = audit_committee_compliance(summary_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7a7e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_acm_urls(flagged_df):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in flagged_df.iterrows():\n",
    "        company = row['corp_name']\n",
    "        corp = row['corp_code']\n",
    "        rcp = row['disclosure'] # update to append the disclosure from exec_df most recent \n",
    "\n",
    "        try:\n",
    "            subdocs = dart.sub_docs(str(rcp))  # rcept_no must be string\n",
    "            match = subdocs[subdocs['title'].str.contains(\"감사제도에 관한 사항\")]\n",
    "\n",
    "            if not match.empty:\n",
    "                url = match.iloc[0]['url']\n",
    "            else:\n",
    "                url = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch for {corp} ({rcp}): {e}\")\n",
    "            url = None\n",
    "\n",
    "        results.append({\n",
    "            'corp_code': str(corp),\n",
    "            'corp_name': company,\n",
    "            'rcept_no': rcp,\n",
    "            'url': url,\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.7)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f927db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_targets_df = missing_acm_urls(flagged).drop_duplicates(subset=['corp_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "321ad5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_update_audit_members(audit_targets_df, exec_df, summary_df):\n",
    "    new_execs_to_add = []\n",
    "    summary_updates = {}\n",
    "\n",
    "    for idx, row in audit_targets_df.iterrows():\n",
    "        corp_code = str(row['corp_code'])\n",
    "        company = row['corp_name']\n",
    "        url = row['url']\n",
    "\n",
    "        if pd.isna(url) or not isinstance(url, str):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            members_found = []\n",
    "            auditors_found = []\n",
    "            \n",
    "            # 1. find a table under a specific title\n",
    "            ac_header_patterns = [\n",
    "                re.compile(r'감사위원\\s*현황'),\n",
    "                re.compile(r'감사위원회\\s*위원의\\s*인적사항'),\n",
    "                re.compile(r'감사위원회\\s*위원'),\n",
    "                re.compile(r'감사기구\\s*관련\\s*사항')\n",
    "            ]\n",
    "            \n",
    "            found_ac_section = None\n",
    "            ac_table = None\n",
    "            for pattern in ac_header_patterns:\n",
    "                found_ac_section = soup.find(string=pattern)\n",
    "                if found_ac_section:\n",
    "                    # find the first table under the header \n",
    "                    ac_table = found_ac_section.find_next('table')\n",
    "                    if ac_table:\n",
    "                        break\n",
    "            \n",
    "            if ac_table:\n",
    "                # parse the target table for columns indicating name and whether or not the listed exec is an outside director\n",
    "                headers = [th.get_text(strip=True).replace('\\xa0', '').replace('\\n', '') for tr in ac_table.find_all('tr', limit=2) for th in tr.find_all(['th', 'td'])]\n",
    "                name_idx = next((i for i, h in enumerate(headers) if '성명' in h), None)\n",
    "                outside_idx = next((i for i, h in enumerate(headers) if '사외이사' in h), None)\n",
    "                \n",
    "                if name_idx is not None and outside_idx is not None:\n",
    "                    # parse the rows of the table\n",
    "                    data_rows = ac_table.find_all('tbody')[0].find_all('tr') if ac_table.find('tbody') else ac_table.find_all('tr')[len(ac_table.find_all('tr', limit=2)):]\n",
    "                    for tr in data_rows:\n",
    "                        tds = tr.find_all(['td', 'th'])\n",
    "                        if len(tds) > max(name_idx, outside_idx):\n",
    "                            name = tds[name_idx].get_text(strip=True)\n",
    "                            is_outside = tds[outside_idx].get_text(strip=True)\n",
    "                            if name and name != '-' and '---' not in name:\n",
    "                                is_outside_flag = '예' in is_outside or 'O' in is_outside\n",
    "                                members_found.append({'name': name, 'is_outside': is_outside_flag})\n",
    "      \n",
    "                                        \n",
    "            # make updates based on extracted info \n",
    "            if members_found:\n",
    "                total_members = len(members_found)\n",
    "                outside_members = sum(1 for member in members_found if member['is_outside'])\n",
    "                \n",
    "                for member in members_found:\n",
    "                    name = member['name']\n",
    "                    existing_mask = (exec_df['corp_code'] == corp_code) & (exec_df['name'] == name)\n",
    "                    if not exec_df[existing_mask].empty:\n",
    "                        exec_df.loc[existing_mask, 'is_audit_committee_member'] = True\n",
    "                    else:\n",
    "                        new_execs_to_add.append({\n",
    "                            'corp_code': corp_code, 'corp_name': company, 'name': name,\n",
    "                            'chrg_job': '감사위원회 위원', 'is_audit_committee_member': True\n",
    "                        })\n",
    "                \n",
    "                summary_updates[corp_code] = {\n",
    "                    'audit_committee': total_members, 'audit_committee_ods': outside_members,\n",
    "                }\n",
    "            else:\n",
    "                summary_updates[corp_code] = {\n",
    "                    'audit_committee': 0, 'audit_committee_ods': 0,\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred for {corp_code} - {company}: {e}\")\n",
    "        time.sleep(0.7)\n",
    "    \n",
    "    for corp_code, update in summary_updates.items():\n",
    "        summary_df.loc[summary_df['corp_code'] == corp_code, 'audit_committee'] = update['audit_committee']\n",
    "        summary_df.loc[summary_df['corp_code'] == corp_code, 'audit_committee_ods'] = update['audit_committee_ods']\n",
    "        \n",
    "    return exec_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46471b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_df, summary_df = parse_and_update_audit_members(\n",
    "    audit_targets_df,\n",
    "    exec_df,\n",
    "    summary_df\n",
    ")\n",
    "\n",
    "updated_flags = audit_committee_compliance(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join('..', 'data', 'processed')\n",
    "exec_file_path = os.path.join(output_folder, 'exec_df.csv')\n",
    "summary_file_path = os.path.join(output_folder, 'summary_df.csv')\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "exec_df.to_csv(exec_file_path, index=False, encoding='utf-8-sig')\n",
    "summary_df.to_csv(summary_file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35008089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last to do: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
